{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJN_uUcB3bl_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, sent_tokenize,pos_tag, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from textblob import Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLseoslh3hlW"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8cRAjAv4aYO"
   },
   "outputs": [],
   "source": [
    "data = data.drop('author', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1573106448060,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "OkaH5uWoU_V_",
    "outputId": "93f3bd1a-dc4b-4a04-d9d9-7df473a1b685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
      " 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"
     ]
    }
   ],
   "source": [
    "print(data.sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSQAs5ac4lAY"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with other emotion label\n",
    "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
    "data = data.drop(data[data.sentiment == 'empty'].index)\n",
    "data = data.drop(data[data.sentiment == 'fun'].index)\n",
    "data = data.drop(data[data.sentiment == 'relief'].index)\n",
    "data = data.drop(data[data.sentiment == 'surprise'].index)\n",
    "data = data.drop(data[data.sentiment == 'worry'].index)\n",
    "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
    "data = data.drop(data[data.sentiment == 'neutral'].index)\n",
    "data = data.drop(data[data.sentiment == 'love'].index)\n",
    "data = data.drop(data[data.sentiment == 'anger'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1573106452206,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "GLYAU_OeV9k7",
    "outputId": "012dcf1a-7b7b-4f89-dbd6-e829770b2788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness' 'hate' 'happiness']\n"
     ]
    }
   ],
   "source": [
    "print(data.sentiment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKl3bL8b4p8x"
   },
   "outputs": [],
   "source": [
    "# Making all letters lowercase\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Removing Punctuation, Symbols\n",
    "data['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# Removing Stop Words using NLTK\n",
    "stop = stopwords.words('english')\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "#Lemmatisation\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# remove any text starting with User... \n",
    "data['content'] = data['content'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "    \n",
    "# remove IP addresses or user IDs\n",
    "data['content'] = data['content'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "    \n",
    "#remove http links in the text\n",
    "data['content'] = data['content'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "#Correcting Letter Repetitions\n",
    "\n",
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQvnqkIq709A"
   },
   "outputs": [],
   "source": [
    "#Encoding output labels 'sadness' as '1' & 'happiness' as '0'\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09Da-Gbt7twZ"
   },
   "outputs": [],
   "source": [
    "# Splitting into training and testing data in 90:10 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1573106460145,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "PlQPr3S0hu0K",
    "outputId": "fcb32331-c61a-4d39-fb39-af600157d74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Term Frequency Matrix:  (10527, 8054)\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "# Extracting features from text files\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_test_counts =count_vect.transform(X_test)\n",
    "print('Shape of Term Frequency Matrix: ',X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKlA9YpQ5Fg1"
   },
   "outputs": [],
   "source": [
    "# Code to find the top 10,000 rarest words appearing in the data\n",
    "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]\n",
    "\n",
    "# Removing all those rarely appearing words from the data\n",
    "freq = list(freq.index)\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTfnkg7770YJ"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SipPgprtF7cK"
   },
   "source": [
    "**Working with TfidVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQnmuZLx-fCL"
   },
   "outputs": [],
   "source": [
    "# Using a pipeline\n",
    "\n",
    "# Logistic Regression pipeline setup\n",
    "logreg_pipe = Pipeline([\n",
    "                     ('tvec', TfidfVectorizer()),\n",
    "                     ('logreg', LogisticRegression())\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74286,
     "status": "ok",
     "timestamp": 1573107074453,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "zQDC-uigHcnp",
    "outputId": "f8c835f8-7716-4a75-9478-ab2969770488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Brian\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Roux-EuDHhB3"
   },
   "outputs": [],
   "source": [
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    " 'tvec__max_features':[100, 2000],\n",
    " 'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    " 'tvec__stop_words': [None, 'english'],\n",
    " \n",
    "}\n",
    "\n",
    "#Logistic Regression params\n",
    "logreg_params = {\n",
    "    'tvec__max_features':[100, 2000],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'logreg__C': [1],\n",
    "    'logreg__solver': ['lbfgs'],\n",
    "    'logreg__multi_class': ['multinomial'],\n",
    "    'logreg__max_iter': [1000],\n",
    "    'logreg__penalty': ['l2']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB9Ayv7HMklt"
   },
   "outputs": [],
   "source": [
    "# #Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Setting up GridSearch for Logistic Regression\n",
    "logreg_gs = GridSearchCV(logreg_pipe, param_grid=logreg_params, cv = 5, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39078,
     "status": "ok",
     "timestamp": 1573109745903,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "Opkhwz5BeUEv",
    "outputId": "cfccfc73-013b-4fde-fa2c-a32ec92ba62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   57.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tvec__max_features': [100, 2000], 'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)], 'tvec__stop_words': [None, 'english'], 'logreg__C': [1], 'logreg__solver': ['lbfgs'], 'logreg__multi_class': ['multinomial'], 'logreg__max_iter': [1000], 'logreg__penalty': ['l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression CV GS\n",
    "logreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1573110514781,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "RrV3KtE5FOPd",
    "outputId": "174d81c6-bacb-4c5b-9383-9c8af00df697"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878787878787878"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring training data on Logistic Regression\n",
    "logreg_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1573110523363,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "x9ZmW7ksFRgx",
    "outputId": "380f53b8-c968-49c3-f525-f65a23a42c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7264957264957265"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring testing data on Logistic Regression\n",
    "logreg_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2KnTE1r63DO"
   },
   "outputs": [],
   "source": [
    "#Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n",
    "\n",
    "tweets = pd.DataFrame(['I am very unhappy today! The atmosphere looks gloom',\n",
    "'Things are looking great. It was such a perfect day',\n",
    "'Success is right around the corner. Lets do this guys',\n",
    "'Everything is more beautiful when you experience them with a smile!',\n",
    "'Now this is my worst, okay? But I am gonna get better.',\n",
    "'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n",
    "'This is quite depressing. I am filled with sorrow',\n",
    "'I am so excited about tonight I cannot wait to get home',\n",
    "'His death broke my heart. It was a sad day',\n",
    "'He makes me so angry sometimes',\n",
    "'I hate working here',\n",
    "'I dislike deal with my workmates anymore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2255,
     "status": "ok",
     "timestamp": 1573119693324,
     "user": {
      "displayName": "Brian Munyi",
      "photoUrl": "",
      "userId": "14053125955485987464"
     },
     "user_tz": -180
    },
    "id": "Xmn84j_V5bgp",
    "outputId": "05ae09d2-8002-4f11-8cae-299ce7737c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     i am very unhappy today! the atmosphere looks ...\n",
      "1     things are looking great. it was such a perfec...\n",
      "2     success is right around the corner. lets do th...\n",
      "3     everything is more beautiful when you experien...\n",
      "4     now this is my worst, okay? but i am gonna get...\n",
      "5     i am tired, boss. tired of being on the road, ...\n",
      "6     this is quite depressing. i am filled with sorrow\n",
      "7     i am so excited about tonight i cannot wait to...\n",
      "8            his death broke my heart. it was a sad day\n",
      "9                        he makes me so angry sometimes\n",
      "10                                  i hate working here\n",
      "11             i dislike deal with my workmates anymore\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets[0].apply(lambda x: x.lower())\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness' 'happiness' 'happiness' 'happiness' 'sadness' 'sadness'\n",
      " 'sadness' 'happiness' 'sadness' 'sadness' 'hate' 'sadness']\n",
      "0     i am very unhappy today! the atmosphere looks ...\n",
      "1     things are looking great. it was such a perfec...\n",
      "2     success is right around the corner. lets do th...\n",
      "3     everything is more beautiful when you experien...\n",
      "4     now this is my worst, okay? but i am gonna get...\n",
      "5     i am tired, boss. tired of being on the road, ...\n",
      "6     this is quite depressing. i am filled with sorrow\n",
      "7     i am so excited about tonight i cannot wait to...\n",
      "8            his death broke my heart. it was a sad day\n",
      "9                        he makes me so angry sometimes\n",
      "10                                  i hate working here\n",
      "11             i dislike deal with my workmates anymore\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "logreg_tfid_tweet = logreg_gs.best_estimator_.predict(tweets)\n",
    "print(lbl_enc.inverse_transform(logreg_tfid_tweet))\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "logreg_classifier = open(\"emotion_logreg.pickle\", 'wb')\n",
    "pickle.dump(logreg_gs, logreg_classifier)\n",
    "logreg_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'sentiment', 'content'], dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion-Analysis2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
